# -*- coding: utf-8 -*-
"""
å¤šä»»åŠ¡ Transformer æ¨¡å‹ï¼ˆè½´æ‰¿æ•…éšœåˆ†ç±» + æŒ¯åŠ¨è¶‹åŠ¿é¢„æµ‹ï¼‰
=========================================================
æœ¬æ–‡ä»¶åŒ…å«ä¸‰å¤§æ”¹è¿›ï¼š
1. **å¤šå°ºåº¦å·ç§¯åµŒå…¥**ï¼ˆMultiâ€‘Scale ConvEmbeddingï¼‰
2. **Tâ€‘PE ä½ç½®å…ˆéªŒåç½®**ï¼ˆTemporal Positional Biasï¼‰
3. **é—¨æ§ç¨€ç–æ³¨æ„åŠ›**ï¼ˆGated Sparse Attentionï¼‰  â† ğŸ†• æœ¬æ¬¡æ–°å¢
   - å±€éƒ¨çª—å£ + å…¨å±€é”šç‚¹æ©ç 
   - ç‚¹ç§¯é˜ˆå€¼é—¨æ§ `Ï„`ï¼Œæ˜¾å¼ç­›é™¤å¼±ç›¸å…³

æ‰€æœ‰ä»£ç å·²ç”¨ä¸­æ–‡æ³¨é‡Šï¼Œä¿®æ”¹æ®µè½ä»¥ `# === æ–°å¢ ===` / `# === ä¿®æ”¹ ===` æ ‡è®°ã€‚
"""
from typing import Tuple, List, Optional, Sequence
import math
import torch
import torch.nn as nn

# ---------------------------------------------------------------------------
# === ä¿ç•™: å¤šå°ºåº¦å·ç§¯åµŒå…¥æ¨¡å— ===
# ---------------------------------------------------------------------------
class MultiScaleConvEmbedding(nn.Module):
    """å¤šå°ºåº¦å·ç§¯åµŒå…¥"""

    def __init__(
        self,
        in_channels: int,
        conv_channels: int,
        kernel_sizes: Tuple[int, ...] | List[int] = (3, 5, 9),
        stride: int = 1,
    ) -> None:
        super().__init__()
        self.paths = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(in_channels, conv_channels, k, stride=stride, padding=k // 2),
                nn.BatchNorm1d(conv_channels),
                nn.ReLU(inplace=True),
            )
            for k in kernel_sizes
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.permute(0, 2, 1)                       # (B, C, L)
        outs = [path(x) for path in self.paths]
        x = torch.cat(outs, dim=1)                   # (B, C*, L)
        return x.permute(0, 2, 1)                    # (B, L, C*)

# ---------------------------------------------------------------------------
# === ä¿ç•™: Tâ€‘PE ä½ç½®å…ˆéªŒåç½® ===
# ---------------------------------------------------------------------------

def build_tpe_bias(
    seq_len: int,
    *,
    sigma: Optional[float] = None,
    period: int = 401,
    bias_strength: float = 0.3,
    device: torch.device | None = None,
) -> torch.Tensor:
    """æ„é€  Tâ€‘PE åç½®çŸ©é˜µ B = B_gauss + B_perã€‚"""
    device = device or torch.device("cpu")
    idx = torch.arange(seq_len, device=device)
    d = (idx.unsqueeze(0) - idx.unsqueeze(1)).abs()
    sigma = sigma or (period / 2)
    gauss = torch.exp(-(d.float() ** 2) / (2 * sigma ** 2))
    periodic = bias_strength * (d % period == 0).float()
    return gauss + periodic                                # (L, L)

# ---------------------------------------------------------------------------
# === ä¿ç•™: æ­£å¼¦ä½ç½®ç¼–ç  ===
# ---------------------------------------------------------------------------
class PositionalEncoding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç """

    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-(torch.log(torch.tensor(10000.0)) / d_model)))
        pe[:, 0::2] = torch.sin(position * div)
        pe[:, 1::2] = torch.cos(position * div)
        self.register_buffer("pe", pe.unsqueeze(0))

    def forward(self, x: torch.Tensor):
        return x + self.pe[:, : x.size(1), :]

# ---------------------------------------------------------------------------
# === æ–°å¢: é—¨æ§ç¨€ç–æ³¨æ„åŠ›æ¨¡å— ===
# ---------------------------------------------------------------------------
class GatedSparseAttention(nn.Module):
    """é—¨æ§ç¨€ç–å¤šå¤´æ³¨æ„åŠ›

    - ä»…è®¡ç®—å±€éƒ¨çª—å£ `w` å†…æˆ–å…¨å±€é”šç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›ã€‚
    - å¯¹å…è®¸ä½ç½®å†ç”¨é˜ˆå€¼ `Ï„` è¿›è¡Œé—¨æ§ï¼Œå»é™¤å¼±ç›¸å…³ã€‚
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        *,
        window_size: int = 5,
        tau: float = 0.0,
        global_indices: Sequence[int] = (0,),  # é»˜è®¤æŠŠé¦– token è®¾ä¸ºå…¨å±€ï¼ˆç±» [CLS]ï¼‰
        dropout: float = 0.1,
    ) -> None:
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim å¿…é¡»èƒ½æ•´é™¤ num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale = math.sqrt(self.head_dim)
        # QKV çº¿æ€§æŠ•å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

        # ç¨€ç–/é—¨æ§å‚æ•°
        self.window_size = window_size
        self.tau = tau
        self.register_buffer("global_mask", self._build_global_mask(global_indices), persistent=False)

    # -----------------------------
    # è¾…åŠ©: æ„é€ å…¨å±€é”šç‚¹å¸ƒå°”å‘é‡
    # -----------------------------
    def _build_global_mask(self, indices: Sequence[int]) -> torch.Tensor:
        mask = torch.zeros(1, dtype=torch.bool)  # placeholder, çœŸæ­£å¤§å°åœ¨ forward æ„é€ 
        if len(indices):
            mask = torch.tensor(indices, dtype=torch.long)
        return mask

    # -----------------------------
    # å‰å‘ä¼ æ’­
    # -----------------------------
    def forward(self, x: torch.Tensor, pos_bias: torch.Tensor) -> torch.Tensor:
        # x: (B, L, D) ; pos_bias: (L, L)
        B, L, _ = x.shape
        H, Dh = self.num_heads, self.head_dim
        device = x.device

        # (B, L, D) â†’ (B, H, L, Dh)
        def _proj(linear, t):
            return linear(t).view(B, L, H, Dh).transpose(1, 2)  # (B, H, L, Dh)

        q = _proj(self.q_proj, x)
        k = _proj(self.k_proj, x)
        v = _proj(self.v_proj, x)

        # ç¼©æ”¾ç‚¹ç§¯
        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale  # (B, H, L, L)

        # ---------- æ„é€ å…è®¸å…³æ³¨æ©ç  (å±€éƒ¨ + å…¨å±€) ----------
        idx = torch.arange(L, device=device)
        dist = (idx.unsqueeze(0) - idx.unsqueeze(1)).abs()          # (L, L)
        local_mask = dist <= self.window_size                       # (L, L)

        if self.global_mask.numel() == 1:                           # æœªæ˜¾å¼è®¾ç½®
            g_idx = torch.tensor([0], device=device)                # é»˜è®¤ CLS=0
        else:
            g_idx = self.global_mask.to(device)
        i_global = torch.zeros(L, dtype=torch.bool, device=device)
        j_global = torch.zeros(L, dtype=torch.bool, device=device)
        i_global[g_idx] = True
        j_global[g_idx] = True
        allowed = local_mask | i_global.unsqueeze(1) | j_global.unsqueeze(0)  # (L, L)
        allowed = allowed.unsqueeze(0).unsqueeze(0)                            # (1,1,L,L)

        # ---------- é˜ˆå€¼é—¨æ§ ----------
        gate = (scores > self.tau).to(scores.dtype)               # (B,H,L,L)

        # å¯¹ä¸å…è®¸çš„ä½ç½®ç½® -infï¼Œå…è®¸ä½† gate=0 ä¹Ÿç½® -inf
        neg_inf = torch.finfo(scores.dtype).min
        scores = scores.masked_fill(~allowed, neg_inf)
        scores = scores.masked_fill((allowed & (gate == 0)), neg_inf)

        # ---------- åŠ ä½ç½®åç½®ï¼ŒSoftmax ----------
        scores = scores + pos_bias.to(scores.dtype).unsqueeze(0).unsqueeze(0)
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        # ---------- åŠ æƒæ±‚å’Œ ----------
        out = torch.matmul(attn, v)                                 # (B,H,L,Dh)
        out = out.transpose(1, 2).reshape(B, L, self.embed_dim)     # (B,L,D)
        return self.out_proj(out)                                   # (B,L,D)

# ---------------------------------------------------------------------------
# === ä¿®æ”¹: Transformer ç¼–ç å™¨å—ï¼Œä½¿ç”¨é—¨æ§ç¨€ç–æ³¨æ„åŠ› ===
# ---------------------------------------------------------------------------
class TransformerEncoderBlock(nn.Module):
    """å•å±‚ Transformer ç¼–ç å™¨ï¼ˆGated Sparse Attention + FFNï¼‰"""

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        ffn_dim: int,
        *,
        window_size: int = 5,
        tau: float = 0.0,
        global_indices: Sequence[int] = (0,),
        dropout: float = 0.1,
    ) -> None:
        super().__init__()
        self.attn = GatedSparseAttention(
            embed_dim,
            num_heads,
            window_size=window_size,
            tau=tau,
            global_indices=global_indices,
            dropout=dropout,
        )
        self.norm1 = nn.LayerNorm(embed_dim)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ffn_dim),
            nn.ReLU(inplace=True),
            nn.Linear(ffn_dim, embed_dim),
        )
        self.norm2 = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, pos_bias: torch.Tensor):
        # ---- é—¨æ§ç¨€ç–æ³¨æ„åŠ› ----
        residual = x
        x = self.norm1(x)
        x = residual + self.dropout(self.attn(x, pos_bias))

        # ---- å‰é¦ˆç½‘ç»œ ----
        residual = x
        x = self.norm2(x)
        x = residual + self.dropout(self.ffn(x))
        return x

# ---------------------------------------------------------------------------
# === ä¿®æ”¹: Transformer Backbone ä¼ é€’ç¨€ç–å‚æ•° ===
# ---------------------------------------------------------------------------
class TransformerBackbone(nn.Module):
    """ç”±è‹¥å¹² `TransformerEncoderBlock` ç»„æˆçš„ç¼–ç å™¨æ ˆ"""

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        num_layers: int,
        ffn_dim: int,
        *,
        window_size: int = 5,
        tau: float = 0.0,
        global_indices: Sequence[int] = (0,),
        dropout: float = 0.1,
    ) -> None:
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerEncoderBlock(
                embed_dim,
                num_heads,
                ffn_dim,
                window_size=window_size,
                tau=tau,
                global_indices=global_indices,
                dropout=dropout,
            )
            for _ in range(num_layers)
        ])

    def forward(self, x: torch.Tensor, pos_bias: torch.Tensor):
        for layer in self.layers:
            x = layer(x, pos_bias)
        return x

# ---------------------------------------------------------------------------
# === ä¿®æ”¹: å¤šä»»åŠ¡æ¨¡å‹ï¼Œæ–°å¢ç¨€ç–æ³¨æ„åŠ›è¶…å‚ ===
# ---------------------------------------------------------------------------
class MultiTaskModel(nn.Module):
    """å¤šä»»åŠ¡ Transformer æ¨¡å‹ï¼ˆåˆ†ç±» + è¶‹åŠ¿é¢„æµ‹ï¼‰"""

    def __init__(
        self,
        *,
        in_channels: int = 1,
        conv_channels: int = 128,
        kernel_sizes: Tuple[int, ...] | List[int] = (3, 5, 9),
        embed_dim: Optional[int] = None,
        num_heads: int = 4,
        num_layers: int = 2,
        ffn_dim: int = 256,
        num_classes: int = 10,
        num_regression: int = 1024,
        dropout: float = 0.1,
        # --- Tâ€‘PE å‚æ•° ---
        period: int = 401,
        sigma: Optional[float] = None,
        bias_strength: float = 0.3,
        # --- ç¨€ç–æ³¨æ„åŠ›å‚æ•° ---
        window_size: int = 5,
        tau: float = 0.0,
        global_indices: Sequence[int] = (0,),
    ) -> None:
        super().__init__()
        self.period = period
        self.sigma = sigma
        self.bias_strength = bias_strength

        # åµŒå…¥å±‚
        self.embedding = MultiScaleConvEmbedding(in_channels, conv_channels, kernel_sizes)
        conv_out_dim = conv_channels * len(kernel_sizes)
        self.embed_dim = embed_dim or conv_out_dim
        self.proj = nn.Identity() if conv_out_dim == self.embed_dim else nn.Linear(conv_out_dim, self.embed_dim)

        # ä½ç½®ç¼–ç  & Backbone
        self.positional_encoding = PositionalEncoding(self.embed_dim)
        self.backbone = TransformerBackbone(
            self.embed_dim,
            num_heads,
            num_layers,
            ffn_dim,
            window_size=window_size,
            tau=tau,
            global_indices=global_indices,
            dropout=dropout,
        )

        # ä»»åŠ¡å¤´
        self.classifier = nn.Linear(self.embed_dim, num_classes)
        self.regressor = nn.Linear(self.embed_dim, num_regression)

    # -----------------------------
    # å‰å‘ä¼ æ’­
    # -----------------------------
    def forward(self, x: torch.Tensor):
        B, L, _ = x.shape
        device = x.device

        # æ„é€  Tâ€‘PE åç½®çŸ©é˜µ
        pos_bias = build_tpe_bias(
            L,
            sigma=self.sigma,
            period=self.period,
            bias_strength=self.bias_strength,
            device=device,
        )  # (L, L)

        # åµŒå…¥ + æŠ•å½±
        x = self.proj(self.embedding(x))
        x = self.positional_encoding(x)

        # å¸¦ç¨€ç–æ³¨æ„åŠ›çš„ Transformer ä¸»å¹²
        x = self.backbone(x, pos_bias)

        pooled = x[:, 0, :]
        return self.classifier(pooled), self.regressor(pooled)

# ---------------------------------------------------------------------------
# å¿«é€Ÿå•å…ƒæµ‹è¯•
# ---------------------------------------------------------------------------
if __name__ == "__main__":
    B, L = 2, 256
    dummy = torch.randn(B, L, 1)
    model = MultiTaskModel()
    cls, reg = model(dummy)
    print(cls.shape, reg.shape)
# Output: torch.Size([2, 10]) torch.Size([2, 1024])
